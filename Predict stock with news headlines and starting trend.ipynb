{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "english_stemmer=nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "from sklearn.feature_selection.univariate_selection import SelectKBest, chi2, f_classif\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.linear_model import SGDClassifier, SGDRegressor,LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import six\n",
    "from abc import ABCMeta\n",
    "from scipy import sparse\n",
    "from scipy.sparse import issparse\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils import check_X_y, check_array\n",
    "from sklearn.utils.extmath import safe_sparse_dot\n",
    "from sklearn.preprocessing import normalize, binarize, LabelBinarizer\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Lambda\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM, SimpleRNN, GRU\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from collections import defaultdict\n",
    "from keras.layers.convolutional import Convolution1D\n",
    "from keras import backend as K\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding a new column for prediction .Start_trend checks previous days closing and current days opening and determines whether its a positive trend or not "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r'C:/usf/Data Science Programming/project/Combined_News_DJIA_with_start.csv')\n",
    "train = data[data['Date'] < '2015-01-01']\n",
    "test = data[data['Date'] > '2014-12-31']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((823, 28), (1166, 28))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape,test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainheadlines = []\n",
    "for row in range(0,len(train.index)):\n",
    "    trainheadlines.append(' '.join(str(x) for x in train.iloc[row,2:27]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(823, 23711)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<823x23711 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 251942 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basicvectorizer = CountVectorizer()\n",
    "basictrain = basicvectorizer.fit_transform(trainheadlines)\n",
    "print(basictrain.shape)\n",
    "basictrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(train[\"start_trend\"])[:,None]\n",
    "basictrain1=sparse.hstack((basictrain,np.array(train[\"start_trend\"])[:,None]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "basicmodel = LogisticRegression()\n",
    "basicmodel = basicmodel.fit(basictrain1, train[\"Label\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logic Regression 1 accuracy:  0.5085763293310464\n"
     ]
    }
   ],
   "source": [
    "testheadlines = []\n",
    "for row in range(0,len(test.index)):\n",
    "    testheadlines.append(' '.join(str(x) for x in test.iloc[row,2:27]))\n",
    "basictest = basicvectorizer.transform(testheadlines)\n",
    "basictest1=sparse.hstack((basictest,np.array(test[\"start_trend\"])[:,None]))\n",
    "preds1 = basicmodel.predict(basictest1)\n",
    "#preds1 = basicmodel.predict(test[\"Start_trend\"][:, np.newaxis])\n",
    "acc1=accuracy_score(test['Label'], preds1)\n",
    "print('Logic Regression 1 accuracy: ',acc1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logic Regression 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<823x676 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 36273 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The counts of phrases with two connected words(exclude words which are too common like \"a\" ,\"an\" ,\"the\" and words too uncommon of which counts are too small )\n",
    "\n",
    "#We delete phrases of which frequency lower than 0.03 or higher than 0.97\n",
    "\n",
    "advancedvectorizer = TfidfVectorizer( min_df=0.03, max_df=0.97, max_features = 200000, ngram_range = (2, 2))\n",
    "advancedtrain = advancedvectorizer.fit_transform(trainheadlines)\n",
    "advancedtrain1=sparse.hstack((advancedtrain,np.array(train[\"start_trend\"])[:,None]))\n",
    "advancedtrain1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logic Regression 2 accuracy:  0.5917667238421955\n"
     ]
    }
   ],
   "source": [
    "advancedmodel = LogisticRegression()\n",
    "advancedmodel = advancedmodel.fit(advancedtrain1, train[\"Label\"])\n",
    "testheadlines = []\n",
    "for row in range(0,len(test.index)):\n",
    "    testheadlines.append(' '.join(str(x) for x in test.iloc[row,2:27]))\n",
    "advancedtest = advancedvectorizer.transform(testheadlines)\n",
    "advancedtest1=sparse.hstack((advancedtest,np.array(test[\"start_trend\"])[:,None]))\n",
    "preds2 = advancedmodel.predict(advancedtest1)\n",
    "acc2=accuracy_score(test['Label'], preds2)\n",
    "\n",
    "print('Logic Regression 2 accuracy: ', acc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logic Regression 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The counts of phrases with three connected words(exclude words which are too common like \"a\" ,\"an\" ,\"the\" and words too uncommon of which counts are too small )\n",
    "\n",
    "advancedvectorizer = TfidfVectorizer( min_df=0.0039, max_df=0.1, max_features = 200000, ngram_range = (3, 3))\n",
    "advancedtrain = advancedvectorizer.fit_transform(trainheadlines)\n",
    "advancedtrain=sparse.hstack((advancedtrain,np.array(train[\"start_trend\"])[:,None]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "advancedmodel = LogisticRegression()\n",
    "advancedmodel = advancedmodel.fit(advancedtrain, train[\"Label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logic Regression 3 accuracy:  0.6063464837049742\n"
     ]
    }
   ],
   "source": [
    "testheadlines = []\n",
    "for row in range(0,len(test.index)):\n",
    "    testheadlines.append(' '.join(str(x) for x in test.iloc[row,3:28]))\n",
    "advancedtest = advancedvectorizer.transform(testheadlines)\n",
    "advancedtest=sparse.hstack((advancedtest,np.array(test[\"start_trend\"])[:,None]))\n",
    "\n",
    "preds3 = advancedmodel.predict(advancedtest)\n",
    "acc3 = accuracy_score(test['Label'], preds3)\n",
    "print('Logic Regression 3 accuracy: ', acc3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "advancedvectorizer = TfidfVectorizer( min_df=0.1, max_df=0.7, max_features = 200000, ngram_range = (1, 1))\n",
    "advancedtrain = advancedvectorizer.fit_transform(trainheadlines)\n",
    "advancedtrain=sparse.hstack((advancedtrain,np.array(train[\"start_trend\"])[:,None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NBayes 1 accuracy:  0.5969125214408233\n"
     ]
    }
   ],
   "source": [
    "advancedmodel = MultinomialNB(alpha=0.01)\n",
    "advancedmodel = advancedmodel.fit(advancedtrain, train[\"Label\"])\n",
    "testheadlines = []\n",
    "for row in range(0,len(test.index)):\n",
    "    testheadlines.append(' '.join(str(x) for x in test.iloc[row,3:28]))\n",
    "advancedtest = advancedvectorizer.transform(testheadlines)\n",
    "advancedtest=sparse.hstack((advancedtest,np.array(test[\"start_trend\"])[:,None]))\n",
    "preds4 = advancedmodel.predict(advancedtest)\n",
    "acc4=accuracy_score(test['Label'], preds4)\n",
    "print('NBayes 1 accuracy: ', acc4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NBayes 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "advancedvectorizer = TfidfVectorizer( min_df=0.03, max_df=0.2, max_features = 200000, ngram_range = (2, 2))\n",
    "advancedtrain = advancedvectorizer.fit_transform(trainheadlines)\n",
    "\n",
    "\n",
    "advancedtrain=sparse.hstack((advancedtrain,np.array(train[\"start_trend\"])[:,None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NBayes 2 accuracy:  0.5737564322469982\n"
     ]
    }
   ],
   "source": [
    "advancedmodel = MultinomialNB(alpha=0.0001)\n",
    "advancedmodel = advancedmodel.fit(advancedtrain, train[\"Label\"])\n",
    "testheadlines = []\n",
    "for row in range(0,len(test.index)):\n",
    "    testheadlines.append(' '.join(str(x) for x in test.iloc[row,2:27]))\n",
    "advancedtest = advancedvectorizer.transform(testheadlines)\n",
    "advancedtest=sparse.hstack((advancedtest,np.array(test[\"start_trend\"])[:,None]))\n",
    "preds5 = advancedmodel.predict(advancedtest)\n",
    "acc5 = accuracy_score(test['Label'], preds5)\n",
    "print('NBayes 2 accuracy: ', acc5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "advancedvectorizer = TfidfVectorizer( min_df=0.01, max_df=0.99, max_features = 200000, ngram_range = (1, 1))\n",
    "advancedtrain = advancedvectorizer.fit_transform(trainheadlines)\n",
    "advancedtrain=sparse.hstack((advancedtrain,np.array(train[\"start_trend\"])[:,None]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bibin\\Anaconda2\\envs\\tf_env\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF 1 accuracy:  0.5102915951972555\n"
     ]
    }
   ],
   "source": [
    "advancedmodel = RandomForestClassifier()\n",
    "advancedmodel = advancedmodel.fit(advancedtrain, train[\"Label\"])\n",
    "testheadlines = []\n",
    "for row in range(0,len(test.index)):\n",
    "    testheadlines.append(' '.join(str(x) for x in test.iloc[row,2:27]))\n",
    "advancedtest = advancedvectorizer.transform(testheadlines)\n",
    "advancedtest=sparse.hstack((advancedtest,np.array(test[\"start_trend\"])[:,None]))\n",
    "preds6 = advancedmodel.predict(advancedtest)\n",
    "acc6 = accuracy_score(test['Label'], preds6)\n",
    "print('RF 1 accuracy: ', acc6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bibin\\Anaconda2\\envs\\tf_env\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF 2 accuracy:  0.5308747855917667\n"
     ]
    }
   ],
   "source": [
    "advancedvectorizer = TfidfVectorizer( min_df=0.03, max_df=0.2, max_features = 200000, ngram_range = (2, 2))\n",
    "advancedtrain = advancedvectorizer.fit_transform(trainheadlines)\n",
    "advancedtrain=sparse.hstack((advancedtrain,np.array(train[\"start_trend\"])[:,None]))\n",
    "\n",
    "advancedmodel = RandomForestClassifier()\n",
    "advancedmodel = advancedmodel.fit(advancedtrain, train[\"Label\"])\n",
    "testheadlines = []\n",
    "for row in range(0,len(test.index)):\n",
    "    testheadlines.append(' '.join(str(x) for x in test.iloc[row,2:27]))\n",
    "advancedtest = advancedvectorizer.transform(testheadlines)\n",
    "advancedtest=sparse.hstack((advancedtest,np.array(test[\"start_trend\"])[:,None]))\n",
    "\n",
    "preds7 = advancedmodel.predict(advancedtest)\n",
    "acc7 = accuracy_score(test['Label'], preds7)\n",
    "print('RF 2 accuracy: ', acc7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "advancedvectorizer = TfidfVectorizer( min_df=0.1, max_df=0.9, max_features = 200000, ngram_range = (1, 1))\n",
    "advancedtrain = advancedvectorizer.fit_transform(trainheadlines)\n",
    "advancedtrain=sparse.hstack((advancedtrain,np.array(train[\"start_trend\"])[:,None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBM 1 accuracy:  0.5574614065180102\n"
     ]
    }
   ],
   "source": [
    "advancedmodel = GradientBoostingClassifier()\n",
    "advancedmodel = advancedmodel.fit(advancedtrain, train[\"Label\"])\n",
    "testheadlines = []\n",
    "for row in range(0,len(test.index)):\n",
    "    testheadlines.append(' '.join(str(x) for x in test.iloc[row,2:27]))\n",
    "advancedtest = advancedvectorizer.transform(testheadlines)\n",
    "advancedtest=sparse.hstack((advancedtest,np.array(test[\"start_trend\"])[:,None]))\n",
    "preds8 = advancedmodel.predict(advancedtest.toarray())\n",
    "acc8 = accuracy_score(test['Label'], preds8)\n",
    "print('GBM 1 accuracy: ', acc8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GBM 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "advancedvectorizer = TfidfVectorizer( min_df=0.02, max_df=0.175, max_features = 200000, ngram_range = (2, 2))\n",
    "advancedtrain = advancedvectorizer.fit_transform(trainheadlines)\n",
    "advancedtrain=sparse.hstack((advancedtrain,np.array(train[\"start_trend\"])[:,None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBM 2 accuracy:  0.5368782161234992\n"
     ]
    }
   ],
   "source": [
    "advancedmodel = GradientBoostingClassifier()\n",
    "advancedmodel = advancedmodel.fit(advancedtrain, train[\"Label\"])\n",
    "testheadlines = []\n",
    "for row in range(0,len(test.index)):\n",
    "    testheadlines.append(' '.join(str(x) for x in test.iloc[row,3:28]))\n",
    "advancedtest = advancedvectorizer.transform(testheadlines)\n",
    "advancedtest=sparse.hstack((advancedtest,np.array(test[\"start_trend\"])[:,None]))\n",
    "preds9 = advancedmodel.predict(advancedtest.toarray())\n",
    "acc9 = accuracy_score(test['Label'], preds9)\n",
    "print('GBM 2 accuracy: ', acc9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic Gradient Descent Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "advancedvectorizer = TfidfVectorizer( min_df=0.2, max_df=0.8, max_features = 200000, ngram_range = (1, 1))\n",
    "advancedtrain = advancedvectorizer.fit_transform(trainheadlines)\n",
    "advancedtrain=sparse.hstack((advancedtrain,np.array(train[\"start_trend\"])[:,None]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bibin\\Anaconda2\\envs\\tf_env\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier 1:  0.5137221269296741\n"
     ]
    }
   ],
   "source": [
    "advancedmodel = SGDClassifier(loss='modified_huber', n_iter=5, random_state=0, shuffle=True)\n",
    "advancedmodel = advancedmodel.fit(advancedtrain, train[\"Label\"])\n",
    "testheadlines = []\n",
    "for row in range(0,len(test.index)):\n",
    "    testheadlines.append(' '.join(str(x) for x in test.iloc[row,2:27]))\n",
    "advancedtest = advancedvectorizer.transform(testheadlines)\n",
    "advancedtest=sparse.hstack((advancedtest,np.array(test[\"start_trend\"])[:,None]))\n",
    "preds10 = advancedmodel.predict(advancedtest.toarray())\n",
    "acc10 = accuracy_score(test['Label'], preds10)\n",
    "print('SGDClassifier 1: ', acc10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGDClassifier 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "advancedvectorizer = TfidfVectorizer( min_df=0.03, max_df=0.2, max_features = 200000, ngram_range = (2, 2))\n",
    "advancedtrain = advancedvectorizer.fit_transform(trainheadlines)\n",
    "advancedtrain=sparse.hstack((advancedtrain,np.array(train[\"start_trend\"])[:,None]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bibin\\Anaconda2\\envs\\tf_env\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier 2:  0.5137221269296741\n"
     ]
    }
   ],
   "source": [
    "advancedmodel = SGDClassifier(loss='modified_huber', n_iter=5, random_state=0, shuffle=True)\n",
    "advancedmodel = advancedmodel.fit(advancedtrain, train[\"Label\"])\n",
    "testheadlines = []\n",
    "for row in range(0,len(test.index)):\n",
    "    testheadlines.append(' '.join(str(x) for x in test.iloc[row,2:27]))\n",
    "advancedtest = advancedvectorizer.transform(testheadlines)\n",
    "advancedtest=sparse.hstack((advancedtest,np.array(test[\"start_trend\"])[:,None]))\n",
    "preds11 = advancedmodel.predict(advancedtest.toarray())\n",
    "acc11 = accuracy_score(test['Label'], preds11)\n",
    "print('SGDClassifier 2: ', acc11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NBSVM(six.with_metaclass(ABCMeta, BaseEstimator, ClassifierMixin)):\n",
    "\n",
    "    def __init__(self, alpha=1.0, C=1.0, max_iter=10000):\n",
    "        self.alpha = alpha\n",
    "        self.max_iter = max_iter\n",
    "        self.C = C\n",
    "        self.svm_ = [] # fuggly\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X, y = check_X_y(X, y, 'csr')\n",
    "        _, n_features = X.shape\n",
    "\n",
    "        labelbin = LabelBinarizer()\n",
    "        Y = labelbin.fit_transform(y)\n",
    "        self.classes_ = labelbin.classes_\n",
    "        if Y.shape[1] == 1:\n",
    "            Y = np.concatenate((1 - Y, Y), axis=1)\n",
    "\n",
    "        # LabelBinarizer().fit_transform() returns arrays with dtype=np.int64.\n",
    "        # so we don't have to cast X to floating point\n",
    "        Y = Y.astype(np.float64)\n",
    "\n",
    "        # Count raw events from data\n",
    "        n_effective_classes = Y.shape[1]\n",
    "        self.class_count_ = np.zeros(n_effective_classes, dtype=np.float64)\n",
    "        self.ratios_ = np.full((n_effective_classes, n_features), self.alpha,\n",
    "                                 dtype=np.float64)\n",
    "        self._compute_ratios(X, Y)\n",
    "\n",
    "        # flugglyness\n",
    "        for i in range(n_effective_classes):\n",
    "            X_i = X.multiply(self.ratios_[i])\n",
    "            svm = LinearSVC(C=self.C, max_iter=self.max_iter)\n",
    "            Y_i = Y[:,i]\n",
    "            svm.fit(X_i, Y_i)\n",
    "            self.svm_.append(svm) \n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        n_effective_classes = self.class_count_.shape[0]\n",
    "        n_examples = X.shape[0]\n",
    "\n",
    "        D = np.zeros((n_effective_classes, n_examples))\n",
    "\n",
    "        for i in range(n_effective_classes):\n",
    "            X_i = X.multiply(self.ratios_[i])\n",
    "            D[i] = self.svm_[i].decision_function(X_i)\n",
    "        \n",
    "        return self.classes_[np.argmax(D, axis=0)]\n",
    "        \n",
    "    def _compute_ratios(self, X, Y):\n",
    "        \"\"\"Count feature occurrences and compute ratios.\"\"\"\n",
    "        if np.any((X.data if issparse(X) else X) < 0):\n",
    "            raise ValueError(\"Input X must be non-negative\")\n",
    "\n",
    "        self.ratios_ += safe_sparse_dot(Y.T, X)  # ratio + feature_occurrance_c\n",
    "        normalize(self.ratios_, norm='l1', axis=1, copy=False)\n",
    "        row_calc = lambda r: np.log(np.divide(r, (1 - r)))\n",
    "        self.ratios_ = np.apply_along_axis(row_calc, axis=1, arr=self.ratios_)\n",
    "        check_array(self.ratios_)\n",
    "        self.ratios_ = sparse.csr_matrix(self.ratios_)\n",
    "\n",
    "        #p_c /= np.linalg.norm(p_c, ord=1)\n",
    "        #ratios[c] = np.log(p_c / (1 - p_c))\n",
    "\n",
    "\n",
    "def f1_class(pred, truth, class_val):\n",
    "    n = len(truth)\n",
    "\n",
    "    truth_class = 0\n",
    "    pred_class = 0\n",
    "    tp = 0\n",
    "\n",
    "    for ii in range(0, n):\n",
    "        if truth[ii] == class_val:\n",
    "            truth_class += 1\n",
    "            if truth[ii] == pred[ii]:\n",
    "                tp += 1\n",
    "                pred_class += 1\n",
    "                continue;\n",
    "        if pred[ii] == class_val:\n",
    "            pred_class += 1\n",
    "\n",
    "    precision = tp / float(pred_class)\n",
    "    recall = tp / float(truth_class)\n",
    "\n",
    "    return (2.0 * precision * recall) / (precision + recall)\n",
    "\n",
    "\n",
    "def semeval_senti_f1(pred, truth, pos=2, neg=0): \n",
    "\n",
    "    f1_pos = f1_class(pred, truth, pos)\n",
    "    f1_neg = f1_class(pred, truth, neg)\n",
    "\n",
    "    return (f1_pos + f1_neg) / 2.0;\n",
    "\n",
    "\n",
    "def main(train_file, test_file, ngram=(1, 3)):\n",
    "    print('loading...')\n",
    "    train = pd.read_csv(train_file, delimiter='\\t', encoding='utf-8', header=0,\n",
    "                        names=['text', 'label'])\n",
    "\n",
    "    # to shuffle:\n",
    "    #train.iloc[np.random.permutation(len(df))]\n",
    "\n",
    "    test = pd.read_csv(test_file, delimiter='\\t', encoding='utf-8', header=0,\n",
    "                        names=['text', 'label'])\n",
    "\n",
    "    print('vectorizing...')\n",
    "    vect = CountVectorizer()\n",
    "    classifier = NBSVM()\n",
    "\n",
    "    # create pipeline\n",
    "    clf = Pipeline([('vect', vect), ('nbsvm', classifier)])\n",
    "    params = {\n",
    "        'vect__token_pattern': r\"\\S+\",\n",
    "        'vect__ngram_range': ngram, \n",
    "        'vect__binary': True\n",
    "    }\n",
    "    clf.set_params(**params)\n",
    "\n",
    "    #X_train = vect.fit_transform(train['text'])\n",
    "    #X_test = vect.transform(test['text'])\n",
    "\n",
    "    print('fitting...')\n",
    "    clf.fit(train['text'], train['label'])\n",
    "\n",
    "    print('classifying...')\n",
    "    pred = clf.predict(test['text'])\n",
    "   \n",
    "    print('testing...')\n",
    "    acc = accuracy_score(test['label'], pred)\n",
    "    f1 = semeval_senti_f1(pred, test['label'])\n",
    "    print('NBSVM: acc=%f, f1=%f' % (acc, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NBSVM 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(823, 533)\n"
     ]
    }
   ],
   "source": [
    "advancedvectorizer = TfidfVectorizer( min_df=0.1, max_df=0.8, max_features = 200000, ngram_range = (1, 1))\n",
    "advancedtrain = advancedvectorizer.fit_transform(trainheadlines)\n",
    "advancedtrain=sparse.hstack((advancedtrain,np.array(train[\"start_trend\"])[:,None]))\n",
    "print(advancedtrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NBSVM 1:  0.548885077186964\n"
     ]
    }
   ],
   "source": [
    "advancedmodel = NBSVM(C=0.01)\n",
    "advancedmodel = advancedmodel.fit(advancedtrain, train[\"Label\"])\n",
    "testheadlines = []\n",
    "for row in range(0,len(test.index)):\n",
    "    testheadlines.append(' '.join(str(x) for x in test.iloc[row,2:27]))\n",
    "advancedtest = advancedvectorizer.transform(testheadlines)\n",
    "advancedtest=sparse.hstack((advancedtest,np.array(test[\"start_trend\"])[:,None]))\n",
    "preds12 = advancedmodel.predict(advancedtest)\n",
    "acc12 = accuracy_score(test['Label'], preds12)\n",
    "print('NBSVM 1: ', acc12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NBSVM 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1611, 613)\n"
     ]
    }
   ],
   "source": [
    "advancedvectorizer = TfidfVectorizer( min_df=0.031, max_df=0.2, max_features = 200000, ngram_range = (2, 2))\n",
    "advancedtrain = advancedvectorizer.fit_transform(trainheadlines)\n",
    "advancedtrain=sparse.hstack((advancedtrain,np.array(train[\"start_trend\"])[:,None]))\n",
    "print(advancedtrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NBSVM 2:  0.548885077186964\n"
     ]
    }
   ],
   "source": [
    "advancedmodel = NBSVM(C=0.01)\n",
    "advancedmodel = advancedmodel.fit(advancedtrain, train[\"Label\"])\n",
    "testheadlines = []\n",
    "for row in range(0,len(test.index)):\n",
    "    testheadlines.append(' '.join(str(x) for x in test.iloc[row,2:27]))\n",
    "advancedtest = advancedvectorizer.transform(testheadlines)\n",
    "advancedtest=sparse.hstack((advancedtest,np.array(test[\"start_trend\"])[:,None]))\n",
    "preds13 = advancedmodel.predict(advancedtest)\n",
    "acc13 = accuracy_score(test['Label'], preds13)\n",
    "print('NBSVM 2: ', acc13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(823, 410)\n"
     ]
    }
   ],
   "source": [
    "#MLP\n",
    "batch_size = 32\n",
    "nb_classes = 2\n",
    "advancedvectorizer = TfidfVectorizer( min_df=0.04, max_df=0.3, max_features = 200000, ngram_range = (2, 2))\n",
    "advancedtrain = advancedvectorizer.fit_transform(trainheadlines)\n",
    "advancedtrain=sparse.hstack((advancedtrain,np.array(train[\"start_trend\"])[:,None]))\n",
    "testheadlines = []\n",
    "for row in range(0,len(test.index)):\n",
    "    testheadlines.append(' '.join(str(x) for x in test.iloc[row,2:27]))\n",
    "advancedtest = advancedvectorizer.transform(testheadlines)\n",
    "advancedtest=sparse.hstack((advancedtest,np.array(test[\"start_trend\"])[:,None]))\n",
    "print(advancedtrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (823, 410)\n",
      "X_test shape: (1166, 410)\n",
      "WARNING:tensorflow:From C:\\Users\\bibin\\Anaconda2\\envs\\tf_env\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\bibin\\Anaconda2\\envs\\tf_env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bibin\\Anaconda2\\envs\\tf_env\\lib\\site-packages\\ipykernel_launcher.py:39: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\bibin\\Anaconda2\\envs\\tf_env\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 699 samples, validate on 124 samples\n",
      "Epoch 1/2\n",
      "699/699 [==============================] - 0s 527us/step - loss: 0.6814 - val_loss: 0.6598\n",
      "Epoch 2/2\n",
      "699/699 [==============================] - 0s 190us/step - loss: 0.6419 - val_loss: 0.6426\n",
      "Generating test predictions...\n"
     ]
    }
   ],
   "source": [
    "X_train = advancedtrain.toarray()\n",
    "X_test = advancedtest.toarray()\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "y_train = np.array(train[\"Label\"])\n",
    "y_test = np.array(test[\"Label\"])\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "\n",
    "# pre-processing: divide by max and substract mean\n",
    "scale = np.max(X_train)\n",
    "X_train /= scale\n",
    "X_test /= scale\n",
    "\n",
    "mean = np.mean(X_train)\n",
    "X_train -= mean\n",
    "X_test -= mean\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "# Here's a Deep Dumb MLP (DDMLP)\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=input_dim))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# we'll use categorical xent for the loss, and RMSprop as the optimizer\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "print(\"Training...\")\n",
    "model.fit(X_train, Y_train, nb_epoch=2, batch_size=16, validation_split=0.15)\n",
    "\n",
    "print(\"Generating test predictions...\")\n",
    "preds14 = model.predict_classes(X_test, verbose=0)\n",
    "acc14 = accuracy_score(test[\"Label\"], preds14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction accuracy:  0.6106346483704974\n"
     ]
    }
   ],
   "source": [
    "print('prediction accuracy: ', acc14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.1\n",
    "maxlen = 200\n",
    "batch_size = 32\n",
    "nb_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bibin\\Anaconda2\\envs\\tf_env\\lib\\site-packages\\keras_preprocessing\\text.py:178: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    }
   ],
   "source": [
    "# vectorize the text samples into a 2D integer tensor\n",
    "tokenizer = Tokenizer(nb_words=max_features)\n",
    "tokenizer.fit_on_texts(trainheadlines)\n",
    "sequences_train = tokenizer.texts_to_sequences(trainheadlines)\n",
    "sequences_test = tokenizer.texts_to_sequences(testheadlines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "X_train shape: (823, 200)\n",
      "X_test shape: (1166, 200)\n"
     ]
    }
   ],
   "source": [
    "print('Pad sequences (samples x time)')\n",
    "X_train = sequence.pad_sequences(sequences_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(sequences_test, maxlen=maxlen)\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bibin\\Anaconda2\\envs\\tf_env\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\bibin\\Anaconda2\\envs\\tf_env\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, dropout=0.2, recurrent_dropout=0.2)`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bibin\\Anaconda2\\envs\\tf_env\\lib\\site-packages\\ipykernel_launcher.py:14: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 823 samples, validate on 1166 samples\n",
      "Epoch 1/3\n",
      "823/823 [==============================] - 9s 11ms/step - loss: 0.6923 - acc: 0.5310 - val_loss: 0.6910 - val_acc: 0.5360\n",
      "Epoch 2/3\n",
      "823/823 [==============================] - 8s 10ms/step - loss: 0.6228 - acc: 0.7351 - val_loss: 0.7165 - val_acc: 0.4897\n",
      "Epoch 3/3\n",
      "823/823 [==============================] - 8s 10ms/step - loss: 0.3867 - acc: 0.9004 - val_loss: 0.7665 - val_acc: 0.5129\n",
      "1166/1166 [==============================] - 2s 2ms/step\n",
      "Test score: 0.7664848624330851\n",
      "Test accuracy: 0.5128644940988073\n",
      "Generating test predictions...\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, dropout=0.2))\n",
    "model.add(LSTM(128, dropout_W=0.2, dropout_U=0.2)) \n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=3,\n",
    "          validation_data=(X_test, Y_test))\n",
    "score, acc = model.evaluate(X_test, Y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n",
    "\n",
    "\n",
    "print(\"Generating test predictions...\")\n",
    "preds15 = model.predict_classes(X_test, verbose=0)\n",
    "acc15 = accuracy_score(test['Label'], preds15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction accuracy:  0.5128644939965694\n"
     ]
    }
   ],
   "source": [
    "print('prediction accuracy: ', acc15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_filter = 120\n",
    "filter_length = 2\n",
    "hidden_dims = 120\n",
    "nb_epoch = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bibin\\Anaconda2\\envs\\tf_env\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\bibin\\Anaconda2\\envs\\tf_env\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", filters=120, kernel_size=2, strides=1, padding=\"valid\")`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, dropout=0.2))\n",
    "# we add a Convolution1D, which will learn nb_filter\n",
    "# word group filters of size filter_length:\n",
    "model.add(Convolution1D(nb_filter=nb_filter,\n",
    "                        filter_length=filter_length,\n",
    "                        border_mode='valid',\n",
    "                        activation='relu',\n",
    "                        subsample_length=1))\n",
    "\n",
    "def max_1d(X):\n",
    "    return K.max(X, axis=1)\n",
    "\n",
    "model.add(Lambda(max_1d, output_shape=(nb_filter,)))\n",
    "model.add(Dense(hidden_dims)) \n",
    "model.add(Dropout(0.2)) \n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bibin\\Anaconda2\\envs\\tf_env\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\bibin\\Anaconda2\\envs\\tf_env\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 823 samples, validate on 1166 samples\n",
      "Epoch 1/1\n",
      "823/823 [==============================] - 3s 3ms/step - loss: 0.6938 - acc: 0.5237 - val_loss: 0.6912 - val_acc: 0.5386\n",
      "1166/1166 [==============================] - 0s 396us/step\n",
      "Test score: 0.6911724919315881\n",
      "Test accuracy: 0.5385934822964218\n",
      "Generating test predictions...\n"
     ]
    }
   ],
   "source": [
    "print('Train...')\n",
    "model.fit(X_train, Y_train, batch_size=32, nb_epoch=1,\n",
    "          validation_data=(X_test, Y_test))\n",
    "score, acc = model.evaluate(X_test, Y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n",
    "\n",
    "\n",
    "print(\"Generating test predictions...\")\n",
    "preds16 = model.predict_classes(X_test, verbose=0)\n",
    "acc16 = accuracy_score(test['Label'], preds16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction accuracy:  0.5385934819897084\n"
     ]
    }
   ],
   "source": [
    "print('prediction accuracy: ', acc16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logic Regression 1 accuracy:  0.5085763293310464\n",
      "Logic Regression 2 accuracy:  0.5917667238421955\n",
      "Logic Regression 3 accuracy:  0.6063464837049742\n",
      "NBayes 1 accuracy:  0.5969125214408233\n",
      "NBayes 2 accuracy:  0.5737564322469982\n",
      "RF 1 accuracy:  0.5102915951972555\n",
      "RF 2 accuracy:  0.5308747855917667\n",
      "GBM 1 accuracy:  0.5574614065180102\n",
      "GBM 2 accuracy:  0.5368782161234992\n",
      "SGDClassifier 1:  0.5137221269296741\n",
      "SGDClassifier 2:  0.5137221269296741\n",
      "NBSVM 1:  0.548885077186964\n",
      "NBSVM 2:  0.548885077186964\n",
      "DL1 prediction accuracy:  0.6106346483704974\n",
      "DL2 prediction accuracy:  0.5128644939965694\n",
      "CNN prediction accuracy:  0.5385934819897084\n"
     ]
    }
   ],
   "source": [
    "print('Logic Regression 1 accuracy: ',acc1 )\n",
    "print('Logic Regression 2 accuracy: ', acc2)\n",
    "print('Logic Regression 3 accuracy: ', acc3)\n",
    "print('NBayes 1 accuracy: ', acc4)\n",
    "print('NBayes 2 accuracy: ', acc5)\n",
    "print('RF 1 accuracy: ', acc6)\n",
    "print('RF 2 accuracy: ', acc7)\n",
    "print('GBM 1 accuracy: ', acc8)\n",
    "print('GBM 2 accuracy: ', acc9)\n",
    "print('SGDClassifier 1: ', acc10)\n",
    "print('SGDClassifier 2: ', acc11)\n",
    "print('NBSVM 1: ', acc12)\n",
    "print('NBSVM 2: ', acc13)\n",
    "print('DL1 prediction accuracy: ', acc14)\n",
    "print('DL2 prediction accuracy: ', acc15)\n",
    "print('CNN prediction accuracy: ', acc16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
